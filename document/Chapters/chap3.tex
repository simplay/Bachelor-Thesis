\section{Implementation}

how to discretize from final derivation to computation?
what do we have to precompute, what during runtime?
how does the final algorithm look like
explain shaders: vertex(geometriy, precomp) - and fragment-shader(in local space-tspace)
how from $cie_xyz$ to $cie_rgb$
how gamma correction
how texturing 
can we do better?
our approaches

TODO: explain that there is the jrtr and the scene code - what are their responsibilities.

shader


show schema: scene data => GPU=[Vertex processing, modeling and viewing transformation => Projection => Rasterization, fragment processing, visibility] => image

In computergraphics, we are interested in synthesizing 2d images from a given scene containing our 3d geometries by using so called shader programs. This process is dentonoted as rendering.
The purpose of shader programs, which are executed directly on the GPU hardware device, is to compute the colorization and illumination of the objects living in our scene. All these computations happen in several stages and depend on the provided scene-input paramteters like the camera, light sources, objects material constants and the desired rendering effect one is interested in to model. The shader stages are implemented sequencially as small little programs, the so called vertex-, geometry- and fragment-shaders. Those stages are applied within the rendering pipeline sequencially. 

Our shaders which we use are written in a highlevel language called GLSL, the OpengGL Shading Language. The decission for using OpenGL has been made since my underlying framework, which is responsible for the precomputation of all scene date, is based on another framework, written in Java using JOGL in oder to communicate with the GPU and is also responsible to precompute all the relevant scene data. This framework, the so called jrtr framework, has been developed as an exercise during the class computer graphics held by M. Zwicker which I attended in autumn 2012. The framework itself has been used and further extended during this thesis quite a lot. All necessary input data required for our java framework in order to perfrom the shading is precomputed by using Matlab. This is basically addressing all the required precomputations for the provided heigh-fields, refering to computation of the inverse two dimensional Fourier transformations which are further explained within this chapter. The matlab scripts themself rely on the provided snake nano-scaled sheds images, taken by AFM.

SHOW ARCHITECTURE GRAPHIC from CG


It's noteworthy that all the vertices are processed within the vertex-shader, whereas the fragement shader's responsibility is to perfrom pixelwise rendering, using the input from the vertex shader. Just remember, fragements are determined by a triple of vertices. hence each pixel has assigned a trilinear interpolated value of all input parameters of its spanning vertices.
Usually, all necessary transformations are applied vertex-wise, considering the vertex-shader as the precomputation stage for the later rendering within the rendering pipeline, in the fragment-shader. In the geometry shader, new vertices around a considered vertex can be created. this is useful for debugging - displaying normals graphically for example.

In this section we are going to explain how to get a fragment-shader from our findings for our BRDF formultion from the last section.  this fragment-shader will render the effect of diffraction on our given geometry pixelwise. Therefore, the quality of diffraction depends on the number of pixels we are going to use for the rendering process and this is directly determined by the resolution of the canvas in which the rendered images are being displayed. 
But, before we can start formulating our fragment-shader we first have to write our vertex shader which does all the precomputations. 
 
By the end of the day we will end up with two different shaders, one which basically samples the whole lambda space using a gaussian window. This shader will be modeling the effect of diffraction completely but will also be rather slow. The other shader will use a gaussian window too but will just use a few wavenumber for the sampling process. Furthermore, this shader will thread specularity seperatly as a special case which will be more like an approximation. 








\subsection{Precomputations in Matlab}
Our first task is to precompute the inverse two dimensional discrete fourier transformations of a given snake shed patch of interest taken by AFM. For that purpose we have written a small matlab script which offers a huge collection of mathematically, nummerically fast and stable alforithmes. Our matlab script reads a given image, which is representing a nano-sclaed heightfield, and computes its inverse two dimensional DFT by using matlab's internel inverse fast fourier transformation function, denoted by $ifft2$. Note that we only require once color channel of the input image since the input image is representing an heightfield, encoded by just one color. Basically, we are interested in computing the $ifft2$ for different powers of the input image since our taylor series approximation for the overall computation relies on this. Keep in mind that taking the fourier transoformation of an arbitrary function will result in a complex valued output which imples that we will get a complex value for each pixel of our input image. Therefore, for each input image we get as many output images, representing the two dimensional inverse fourier transformation, as the minimal amount of taylor terms required for a well-enough approximation. In order to store our output images, we have to use 2 color channels instead just one like it was for the given input image. As an optimization step, we do not directly store images, rather we output binary files which contain all RGB values for each pixel in a row first, coloumn last format. This allows us to have much higher precission for the output values and also it does not waste any color channels. Note that we have scaled each pixel value in a range between 0 and 1. Therefore, we have to remember store four scaling factors for each output image as well, which are the real and imaginary minimum and maximum values. Later, using linear interpolation within the shaderm we will get back the image's original values. 


Say something about shifting - the centering why required?
list code


\subsection{Our Java Renderer}
\subsubsection{Scene}
explain geometry computation
explain light(source) setup
explain factories
explain camera setup
explain how materials are stored
explain how assigned to jrtr



\subsubsection{jrtr Framework}

rendering synthesis of 2d images from 3d scene description. rendering algorithms interpret data structures that represent scenes using geometric primitives, matrial properties and lights.
Input: Data structures that represent scene (geometry, material properties, lights, virtual camera)
Output: 2d images (array of pixels - RGB for each pixel)

focus of computer graphics: interactive rendering in order to produce images within a short periode of time which should be as photorealistic as possible - depending on applied shading apporach.


in class(framework) - build own 3d rendering engine

rendering 3d models: camera simulation, interactive viewing, lighting, shading. 
modeling: triangle meshes, smotth surfaces
java base code, openGL 3.0, 



explain how this will work
explain how passed to glsl shader - see computer graphics slides
maybe show schematically the architecture

\subsection{GLSL Diffraction Shader}
explain vertex shader and fragment shader and how they are related.
show rendering pipeline image

review fragment- and vertex-shader

start using the final findings from chapter 2 and substitute
explain how all the components are computed and why they are computed like this.

\subsubsection{Vertex Shader}
The first computional stage within our rendering pipeline is computing all necessary per vertex data. Those computations are preformed in the vertex shader. In our case, we compute for any vertex of our current geometry the direction vectors $k1$ and $k2$ described like previousely in the tangent space. Initially all input data lives in its own space. Hence, we first have to transfrom all input data into the same space in order to use it for later computations within the fragment shader. We are going to transform k1 and k2 into the so called tangent space which. Furthermore, we have also to realign our local coordinate system. This is why there is an rodrigues rotation also involved. In order to avoid scaling issues and since we are only interested in the direction of the vectors k1 and k2, we have to normalize them, too. Last, we also output the position of the current vertex transfomed into the projective camera space.
  
explain $cop_w$
modelM
other shader assigned inputs

\begin{algorithm}
  \caption{Vertex diffraction shader}
  \begin{algorithmic}
    \ForAll{$Vertex \thinspace v \in Shape$}
      \State $ vec3 N = normalize(modelM * vec4(normal,0.0)).xyz$
      \State $ vec3 T = normalize(modelM * vec4(tangent,0.0)).xyz$
      \State $ T = rotateRodrigues(T, N, phi)$
      \State $ vec3 B = normalize(cross(N, T))$
      \State $ vec3 Pos = ((cop_w-position)).xyz$
      \State $ vec4 lightDir = (directionArray[0])$
      \State $ lightDir = normalize(lightDir)$
      \State $ l = projectVectorOnTo(lightDir, TangentSpace)$
      \State $ p = projectVectorOnTo(Pos, TangentSpace)$
      \State $normalize(l); normalize(p)$
      \State $gl_Position = projection * modelview * position$
    \EndFor
  \end{algorithmic}
\end{algorithm}


\subsubsection{Fragment Shader}

sdfsdfgdfgd

\begin{algorithm}
  \caption{Fragment diffraction shader}
  \begin{algorithmic}
    \ForAll{$Pixel \thinspace p \in Fragment$}
      \State \init $BRDF_{XYZ}, BRDF_{RGB}$ \to $vec4(0.0)$
      \State $(u,v,w) = \hat{\mathbf{k_1}}-\hat{\mathbf{k_2}}$
      \For{$(\lambda = \lambda_{min};\thinspace \lambda \leq \lambda_{max};\thinspace \lambda = \lambda + \lambda_{step})$}
        \State $xyzWeights = getClrMatchingFnWeights(\lambda)$
        \State $lookupCoord = getLookupCoord(u, v, \lambda)$
        \State \init $P$ \to $vec2(0.0)$
        \State $k = \frac{2\pi}{\lambda}$
        \For{$(n = 0$ \to $MAXTAYLORTERMS)$}
          \State $taylorScaleF = \frac{(kw)^n}{n!}$
          \State \init $F_{fft}$  \to $vec2(0.0)$
          \State $anchorX = int(floor(center.x + lookupCoord.x * fftImWidth)$
          \State $anchorY = int(floor(center.y + lookupCoord.y * fftImHeight)$
          \For{$(i=(anchorX-winW)$ \to $(anchorX + winW + 1))$}
            \For{$(j=(anchorY - winW)$ \to $(anchorY + winW + 1))$}
              \State $dist = getDistVecFromOriginFor(i,j)$
              \State $position = getLocalLookUp(i,j,n)$
              \State $fftVal = getRescaledFourierTextureValueAt(position)$
              \State $fftVal \asteq getGaussWeightAtDistance(dist)$
              \State $F_{fft} \pluseq fftVal$
            \EndFor
          \EndFor
          \State $P \pluseq taylorScaleF*F_{fft}$
        \EndFor
        \State $xyzPixelColor \pluseq dot(vec3(\left|P\right|^2), xyzWeights)$
      \EndFor
      \State $BRDF_{XYZ} = xyzPixelColor*C(\hat{\mathbf{k_1}},\hat{\mathbf{k_2}})*shadowF$
      \State $BRDF_{RGB}.xyz = D_{65}*M_{XYZ-RGB}*BRDF_{XYZ}.xyz$
      \State $BRDF_{RGB}= gammaCorrect(BRDF_{RGB})$
    \EndFor
  \end{algorithmic}
\end{algorithm}



tell how we are going to sample - uniformly along lambda - explain draw-back of this approach - explain possible solutions for this issue. maybe refer to reference shader or leave this for the disscusion part.