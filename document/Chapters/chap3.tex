\section{Implementation}

how to discretize from final derivation to computation?
what do we have to precompute, what during runtime?
how does the final algorithm look like
explain shaders: vertex(geometriy, precomp) - and fragment-shader(in local space-tspace)
how from $cie_xyz$ to $cie_rgb$
how gamma correction
how texturing 
can we do better?
our approaches

TODO: explain that there is the jrtr and the scene code - what are their responsibilities.

shader


show schema: scene data => GPU=[Vertex processing, modeling and viewing transformation => Projection => Rasterization, fragment processing, visibility] => image

In computergraphics, we are interested in synthesizing 2d images from a given scene containing our 3d geometries by using so called shader programs. This process is dentonoted as rendering.
The purpose of shader programs, which are executed directly on the GPU hardware device, is to compute the colorization and illumination of the objects living in our scene. All these computations happen in several stages and depend on the provided scene-input paramteters like the camera, light sources, objects material constants and the desired rendering effect one is interested in to model. The shader stages are implemented sequencially as small little programs, the so called vertex-, geometry- and fragment-shaders. Those stages are applied within the rendering pipeline sequencially. 

Our shaders which we use are written in a highlevel language called GLSL, the OpengGL Shading Language. The decission for using OpenGL has been made since my underlying framework, which is responsible for the precomputation of all scene date, is based on another framework, written in Java using JOGL in oder to communicate with the GPU and is also responsible to precompute all the relevant scene data. This framework, the so called jrtr framework, has been developed as an exercise during the class computer graphics held by M. Zwicker which I attended in autumn 2012. The framework itself has been used and further extended during this thesis quite a lot. All necessary input data required for our java framework in order to perfrom the shading is precomputed by using Matlab. This is basically addressing all the required precomputations for the provided heigh-fields, refering to computation of the inverse two dimensional Fourier transformations which are further explained within this chapter. The matlab scripts themself rely on the provided snake nano-scaled sheds images, taken by AFM.

SHOW ARCHITECTURE GRAPHIC from CG


It's noteworthy that all the vertices are processed within the vertex-shader, whereas the fragement shader's responsibility is to perfrom pixelwise rendering, using the input from the vertex shader. Just remember, fragements are determined by a triple of vertices. hence each pixel has assigned a trilinear interpolated value of all input parameters of its spanning vertices.
Usually, all necessary transformations are applied vertex-wise, considering the vertex-shader as the precomputation stage for the later rendering within the rendering pipeline, in the fragment-shader. In the geometry shader, new vertices around a considered vertex can be created. this is useful for debugging - displaying normals graphically for example.

In this section we are going to explain how to get a fragment-shader from our findings for our BRDF formultion from the last section.  this fragment-shader will render the effect of diffraction on our given geometry pixelwise. Therefore, the quality of diffraction depends on the number of pixels we are going to use for the rendering process and this is directly determined by the resolution of the canvas in which the rendered images are being displayed. 
But, before we can start formulating our fragment-shader we first have to write our vertex shader which does all the precomputations. 
 
By the end of the day we will end up with two different shaders, one which basically samples the whole lambda space using a gaussian window. This shader will be modeling the effect of diffraction completely but will also be rather slow. The other shader will use a gaussian window too but will just use a few wavenumber for the sampling process. Furthermore, this shader will thread specularity seperatly as a special case which will be more like an approximation. 








\subsection{Precomputations in Matlab}
purpose
explain matlab code
explain shifts
explain what will be outputed

\subsection{Our Java Renderer}
\subsubsection{Scene}
explain geometry computation
explain light(source) setup
explain factories
explain camera setup
explain how materials are stored
explain how assigned to jrtr



\subsubsection{jrtr Framework}

rendering synthesis of 2d images from 3d scene description. rendering algorithms interpret data structures that represent scenes using geometric primitives, matrial properties and lights.
Input: Data structures that represent scene (geometry, material properties, lights, virtual camera)
Output: 2d images (array of pixels - RGB for each pixel)

focus of computer graphics: interactive rendering in order to produce images within a short periode of time which should be as photorealistic as possible - depending on applied shading apporach.


in class(framework) - build own 3d rendering engine

rendering 3d models: camera simulation, interactive viewing, lighting, shading. 
modeling: triangle meshes, smotth surfaces
java base code, openGL 3.0, 



explain how this will work
explain how passed to glsl shader - see computer graphics slides
maybe show schematically the architecture

\subsection{GLSL Diffraction Shader}
explain vertex shader and fragment shader and how they are related.
show rendering pipeline image

review fragment- and vertex-shader

start using the final findings from chapter 2 and substitute
explain how all the components are computed and why they are computed like this.

\subsubsection{Vertex Shader}

\begin{algorithm}
  \caption{Vertex diffraction shader}
  \begin{algorithmic}
    \ForAll{$Vertex \thinspace v \in Shape$}
    \EndFor
  \end{algorithmic}
\end{algorithm}


\subsubsection{Fragment Shader}


describe every helper method how it work

pewpewpew

\begin{algorithm}
  \caption{Fragment diffraction shader}
  \begin{algorithmic}
    \ForAll{$Pixel \thinspace p \in Fragment$}
      \State \init $BRDF_{XYZ}, BRDF_{RGB}$ \to $vec4(0.0)$
      \State $(u,v,w) = \hat{\mathbf{k_1}}-\hat{\mathbf{k_2}}$
      \For{$(\lambda = \lambda_{min};\thinspace \lambda \leq \lambda_{max};\thinspace \lambda = \lambda + \lambda_{step})$}
        \State $xyzWeights = getClrMatchingFnWeights(\lambda)$
        \State $lookupCoord = getLookupCoord(u, v, \lambda)$
        \State \init $P$ \to $vec2(0.0)$
        \State $k = \frac{2\pi}{\lambda}$
        \For{$(n = 0$ \to $MAXTAYLORTERMS)$}
          \State $taylorScaleF = \frac{(kw)^n}{n!}$
          \State \init $F_{fft}$  \to $vec2(0.0)$
          \State $anchorX = int(floor(center.x + lookupCoord.x * fftImWidth)$
          \State $anchorY = int(floor(center.y + lookupCoord.y * fftImHeight)$
          \For{$(i=(anchorX-winW)$ \to $(anchorX + winW + 1))$}
            \For{$(j=(anchorY - winW)$ \to $(anchorY + winW + 1))$}
              \State $dist = getDistVecFromOriginFor(i,j)$
              \State $position = getLocalLookUp(i,j,n)$
              \State $fftVal = getRescaledFourierTextureValueAt(position)$
              \State $fftVal \asteq getGaussWeightAtDistance(dist)$
              \State $F_{fft} \pluseq fftVal$
            \EndFor
          \EndFor
          \State $P \pluseq taylorScaleF*F_{fft}$
        \EndFor
        \State $xyzPixelColor \pluseq dot(vec3(\left|P\right|^2), xyzWeights)$
      \EndFor
      \State $BRDF_{XYZ} = xyzPixelColor*C(\hat{\mathbf{k_1}},\hat{\mathbf{k_2}})*shadowF$
      \State $BRDF_{RGB}.xyz = D_{65}*M_{XYZ-RGB}*BRDF_{XYZ}.xyz$
      \State $BRDF_{RGB}= gammaCorrect(BRDF_{RGB})$
    \EndFor
  \end{algorithmic}
\end{algorithm}



tell how we are going to sample - uniformly along lambda - explain draw-back of this approach - explain possible solutions for this issue. maybe refer to reference shader or leave this for the disscusion part.