\section{Implementation}

how to discretize from final derivation to computation?
what do we have to precompute, what during runtime?
how does the final algorithm look like
explain shaders: vertex(geometriy, precomp) - and fragment-shader(in local space-tspace)
how from $cie_xyz$ to $cie_rgb$
how gamma correction
how texturing 
can we do better?


shader

In computergraphics, we are interested in rendering a given scene containing our 3d geometries by using so called shader programs. 
The purpose of such programs, which run directly on the gpu hardware device, is to compute the colorization and illumination of the objects living in our scene. This computation happens in several stages and depends on the provided input paramteters like the camera, light sources, objects material constants and the desired rendering effect one is interested in to model. The shader stages are also modeled as small little programs, the so called vertex-, geometry- and fragment-shaders. Those stages are applied within the rendering pipeline sequencially. 

Our shaders are written in GLSL, developed for OpenGl. The decission for using OpenGl has been made since the underlying framework which is responsible for the precomputation of all scene date is based on a framework written in Java using JOGL in oder to communicate with the GPU and precompute all the relevant scene data. This framework, the so called jrtr framework has been developed as an exercise during the class computer graphics held by M. Zwicker which I attended in autumn 2012. The framework itself has been extended during this thesis quite a lot. Further, there are also some precomputations involved, perfomed in matlab. This is basically addressing all the required precomputations for the provided heigh-fields, refering to computation of the inverse two dimensional Fourier transformations which are further explained within this chapter.  

It's noteworthy that all the vertices are processed within the vertex-shader, whereas the fragement shader's responsibility is to perfrom pixelwise rendering, using the input from the vertex shader. Just remember, fragements are determined by a triple of vertices. hence each pixel has assigned a trilinear interpolated value of all input parameters of its spanning vertices.
Usually, all necessary transformations are applied vertex-wise, considering the vertex-shader as the precomputation stage for the later rendering within the rendering pipeline, in the fragment-shader. In the geometry shader, new vertices around a considered vertex can be created. this is useful for debugging - displaying normals graphically for example.

In this section we are going to explain how to get a fragment-shader from our findings for our brdf formultion from the last section.  this fragment-shader will render the effect of diffraction on our given geometry pixelwise. Therefore, the quality of diffraction depends on the number of pixels we are going to use for the rendering process and this is directly determined by the resolution of the canvas in which the rendered images are being displayed. 
But, before we can start formulating our fragment-shader we first have to write our vertex shader which does all the precomputations. 
 




\subsection{Setup}
\subsection{Precomputations in Matlab}
\subsection{jrtr Framework}
\subsection{GLSL Diffraction Shader}


\begin{algorithm}
  \caption{Vertex diffraction shader}
  \begin{algorithmic}
    \ForAll{$Vertex \thinspace v \in Shape$}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Fragment diffraction shader}
  \begin{algorithmic}
    \ForAll{$Pixel \thinspace p \in Fragment$}
      \State $BRDF_{XYZ}, BRDF_{RGB} = vec4(0.0)$
      \State $(u,v,w) = \hat{\mathbf{k_1}}-\hat{\mathbf{k_2}}$
      \For{$(\lambda = \lambda_{min};\thinspace \lambda \leq \lambda_{max};\thinspace \lambda = \lambda + \lambda_{step})$}
        \State $k = \frac{2\pi}{\lambda}$
        \State $(w_u, w_v) = (ku, kv)$
        \State $w_{color} = (S_x(\lambda), S_y(\lambda), S_z(\lambda))$
        \For{$(r)$}
          \For{$(s)$}
            \State $coords = getLookUpCoord(r, s)$
            \State $P = taylorApprox(coords, k, w)$
            \State $w_{r,s} = gaussianWeight(dist)$
            \State $scale_{pq} = pqFactor(w_u, w_v)$
            \State $P *= scale_{pq}$
            \State $P_{abs} = \left|P\right|^2$
            \State $P_{abs} *= w_{r,s}$
            \State $BRDF_{XYZ} += vec4(P_{abs}*w_{color}, 0.0)$
          \EndFor
        \EndFor
      \EndFor
      \State $BRDF_{XYZ} = BRDF_{XYZ}*C(\hat{\mathbf{k_1}},\hat{\mathbf{k_2}})*shadowF$
      \State $BRDF_{XYZ}.xyz = D_{65}*M_{XYZ-RGB}*BRDF_{XYZ}.xyz$
      \State $BRDF_{RGB}.xyz = D_{65}*M_{XYZ-RGB}*BRDF_{XYZ}.xyz$
      \State $BRDF_{RGB}= gammaCorrect(BRDF_{RGB})$
    \EndFor
  \end{algorithmic}
\end{algorithm}