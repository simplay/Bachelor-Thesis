\chapter{Theoretical Background}
\section{Basics in Modeling Light in Computer Graphics}

\subsection{Radiometry}
One purpose of Computer Graphics is to simulate the interaction of light on a surface and how a real-world observer, such as a human eye, will perceive this. These visual sensations of an eye are modeled relying on a virtual camera which captures the emitted light from the surface. The physical basis to measure such reflected light depicts radiometry which is about measuring the electromagnetic radiation transfered from a soruce to a receiver. \\

Fundamentally, light is a form of energy propagation, consisting of a large collection of photons, whereat each photon can be considered as a quantum of light that has a position, direction of propagation and a wavelength $\lambda$. A photon travels at a certain speed $v = \frac{c}{n}$, that depends only the speed of light $c$ and the refractive index $n$ through which it progrates. Its frequency is defined by $f = \frac{v}{\lambda}$ and its carried amount of energy $q$, mearsured in the SI unit Joule, is given by $q = hf= \frac{hv}{\lambda n}$ where $h$ is the Plank's constant. The total energy of a large collection of photons is hence $Q = \sum_i q_i$.

\subsection{Spectral Energy}

It is important to understand that the human eye is not equally sensitive to all wavelength of the spectrum of light and therefore responds differently to specific wavelengths. Remember that our goal is to model the human visual perception. This is why we consider the energy distribution of a light spectrum rather than considering the total energry of a photon collection since then we could weight the distribution according the human visual system. So the question we want to answer is: How is the energy distributed across wavelengths of light? \\

The idea is to make an energy histrogram from a given photon collection. For this we have to order all photons by their associated wavelength, discretize wavelength spectrum, count all photons which then will fall in same wavelength-interval, and then, finally, normalize each interval by the total energy $Q$. This will give us a histogram which tells us the spectral energy $Q_{\lambda}$ for a given discrete $\lambda$ interval and thus models the so called spectral energy distribution $\footnote{Intensive quantities can be thought of as density functions that tell the density of an extensive quantity at an infinitesimal point.}$.

\subsection{Spectral Power}
Rendering an image in Computer Graphics corresponds to capturing the color sensation of an illuminated, target scene at a certain point in time. As previousely seen, each color is associated by a wavelength and is directly related to a certain amount of enegry. In order to determine the color of a to-be-rendered pixel of an image, we have to get a sense of how much light (in terms of energy) passes through the area which the pixel corresponds to. One possibility is to consider the flow of energy $\Phi = \frac{\Delta Q}{\Delta t}$ transferred through this area over a small period of time. This allows us to measure the energy flow through a pixel during a certain amount of time. \\

In general, power is the estimated rate of energy production for light sources and corresponds to the flux. It is measured in the unit Watts, denoted by Q. Since power is a rate over time, it is well defined even when energy production is varying over time. As with Spectral Energy for rendering, we are really interested in the spectral power $\Phi_\lambda = \frac{\Phi}{\lambda}$, measured in Watts per nanometer.

\subsection{Spectral Irradiance}
Before we can tell how much light is reflected from a given point on a surface towards the viewing direction of an observer, we first have to know how much light arrives at this point. Since in general a point has no length, area or even volume associated, let us instead consider an infinitimal area $\Delta A$ around a such a point. Then, we can ask ourself how much light falls in such a small area. When further observing this process over a short period in time, this quantity is the spectral irradiance $E$ as illustrated in figure $\ref{fig:irradiance}$. Summarized, this quantity tells us how much spectral power is incident on a surface per unit area and mathematically is equal:

\begin{equation}
 E = \frac{\Phi_{\lambda}}{\Delta A}
\end{equation} 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{background/irradiance.png}
  \caption{Irradiance is the summed up radiance over all directions}
  \label{fig:irradiance}
\end{figure}

\subsection{Spectral Radiance}
When rendering an image we have to determine the color of each pixel of the image. Although irradiance tells us how much light is arriving at a point as illustrated in figure $\ref{fig:irradiance}$, it tells us little about the direction that light comes from. This relates to how the human eye perceives the brightness of an illuminated objects when looking at it in a certain direction. 

\begin{figure}[H]
  \centering
  \subfigure[Radiances is the density of photons per area per solid angle]{
    \includegraphics[scale=0.6]{background/radiancehemisphere.png}
    \label{fig:radiance}
  }
~
  \subfigure[Solid angle is the area of a surface patch on a sphere with radius R which is spanned by a set of directions]{  
    \includegraphics[scale=0.42]{background/solidangle.png}
    \label{fig:solidangle}
  }
  
\label{fig:radianceBasics}
\end{figure}

This concepted is described by the radiometric quantity radiance. Basically, this is a measure of light energy passing through or is emitted off from a small area around a point on a surface towards a given direction during a short period in time. More formally this is the spectral power emerging from an arbitrary point (an infinitimal area around this point) and falls within a given solid angle (see figure$\footnote{Similar figute like used in computer graphics class 2012 in chapter colors}$ $\ref{fig:solidangle}$)in specific direction (usually towards the observer) as shown in figure $\ref{fig:radiance}$. Formally, this leads us to the following mathematical formalism: 

\begin{equation}
 L_{\lambda}(\omega) = \frac{d^2 \Phi_{\lambda}}{dA d\Omega} \approx \frac{\Phi_{\lambda}}{\Omega A}
\end{equation}

where $L$ is the observed spectral radiance in the unit energy per unit area per solid angle, which is $Wm^-2 sr^-1$ in direction $\omega$ which has an angle $\theta$ between the surface normal and $\omega$, $\Theta$ is the total flux or power emitted, $\theta$ is the angle between the surface normal and the specified direction, $A$ is the area of the surface and $\Omega$ is the solid angle in the unit steradian subtended by the observation or measurement. \\

It is useful to distinguish between radiance incident at a point on a surface and excitant from that point. Terms for these concepts sometimes used in the graphics literature are surface radiance $L_r$ for the radiance \textit{reflected} from a surface and field radiance $L_i$ for the radiance \textit{incident} at a surface.  

\subsection{BRDF}
In order to render the colorization of an observed object, a natural question in computer graphics is what portion of the reflected, incident light a viewer will receive, when he looks at an illuminated object. Therefore for any given surfaces which is illuminated from a certain direction $\omega_i$, we can ask ourself how much light is reflected off of any point on this surface towards a viewing direction $\omega_r$. This is where the Bidirectional Reflectance Distribution Function (short: BRDF) comes into play, which is a radiometric quantity telling us how much light is reflected at an opaque surface. Mathematically speaking, the BRDF is the ratio of the reflected radiance pointing to the direction $\omega_r$ to the incident irradiance comming from the inverse direction of $\omega_i$ as illustrated in figure $\ref{fig:brdfillustration}$. Hence the BRDF is a four dimensional function defined by four angles $\theta_i$, $\phi_i$, $\theta_r$ and $\phi_r$.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.5]{background/mybrdfmodel.png}
  \caption[BRDF Model]{Illistration of the BRDF model, where $\omega_i$ is pointing to the light source and the existing direction is denoted by $\omega_r$. Both direction unit direction vectors defined w.r.t to a surface normal $\mathbf{n}$ for every point on the surface.}
  \label{fig:brdfillustration}  
\end{figure}

Which formally is for any given wavelength $\lambda$ equivalent to:

\begin{align}
  BRDF_{\lambda}(\omega_i, \omega_r)
  & = \frac{dL_r(\omega_r)}{dE_i(\omega_i)} \nonumber \\
  & = \frac{dL_r(\omega_r)}{L_i(\omega_i)cos(\theta_i)d\omega_i}
  \label{eq:defbrdf}
\end{align}

Where $L_{r}$ is the reflected spectral radiance, $E_i$ is the spectral irradiance and $\theta_{\text{i}}$ is the angle between $\omega_{\text{i}}$ and the surface normal $\mathbf n$. 

\subsection{Wavespectrum and Colors}
In order to see how crucial the role of human vision plays, let us consider the following definition of color by \textit{Wyszechkiu and Siles}$\footnote{mentioned in Computer Graphics Fundamentals Book from the year 2000}$ stating that \textit{Color is the aspect of visual perception by which an observer may distinguish differences between two structure-free fields of view of the same size and shape such as may be caused by differences in the spectral composition of the radiant energy concerned in the observation}. Therefore, similarly like the humans' perceived sensation of smell and taste, color vision is just another individual sense of perception giving us the ability to distinguish different frequency distribution of light experienced as color.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{background/lightspec.png}
  \caption[visiblelightspectrum]{Frequency (top) and wavelenght (bottom) of colors of the visible light spectrum$\footnotemark$.}
  \label{fig:colorspectrum}
\end{figure}
\footnotetext{Similar figute like used in computer graphics class 2012 in chapter colors}

In general an eye consists of photoreceptor cells which are responsible for providing ability of color-perception. A schematic of an eye is illustratied in figure $\ref{fig:humaneye}$. Basically, there are two specialized types of photoreceptor cells, cone cells which are responsible for color vision and rod cells, which allow an eye to perceive different brigthness levels.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{background/humaneye.png}
  \caption[humanayeschematic]{Schematic$\footnotemark$ of photoreceptor cells, cones and rods, in human eye }
  \label{fig:humaneye}
\end{figure}
\footnotetext{image of illustration has been taken from \href{http://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function}{wikipedia}}

A human eye is made of three different types of cone cells, having their peak sensivity in sensing color at different wavelength ranges. More precisely, there are cone cells most sensitive to short wavelengths which are between $420 nm$ and $440 nm$, those which are most sensitive in the middle range between $530 nm$ and $550 nm$ and those which have their peak in the long range, from $560 nm$ to $580 nm$. In principle, any color sensation in human color perception as shown in figure $\ref{fig:colorspectrum}$ can therefore be described by just three parameters, corresponding to levels of stimulus of the three types of cone cells.  

\subsection{Colorspace}
\label{sec:colorspace}
In order to render accurately images of how a human observer sees its world, a mathematical model of the human color perception is required. Remember that color sensation is due to a visual stimulus processed by cone cells in an eye. A human eye contains three different types of cone cells. Therefore, one possible approach is to describe each kind of these cone cells as a function of wavelength, returning a certain intensivity. In the early 1920, from a series of experiments the so called CIE XYZ color space was derived, describing response of cone cells of an average human individual, the so called standard observer. Basically, a statistically sufficiently large number of probands were exposed to different target light colors expressed by their wavelength. The task of each proband was to reproduce these target colors by mixing three given primary colors, red-, green- and blue-light. The strength  of each primary color could be manually adjusted by setting their relative intensivity. Those adjustment weights have been measured, aggregated and averaged among all probands for each primary color. This model describes each color as a triple of three real valued numbers$\footnote{note that there are  negative color weights possible in the CIE XYZ colors space. This is why some human perceived color sensations could not be reconstructed using just an additive color model (adding three positively weighted primary values). Therefore, a probabant was also allowed to move one of the primary colors to the target color and instead was supposed to reproduce this new color mix using the two remaining primaries (subtractive model). The value of the selected, moved primary was then interpreted as beeing negative weighted in an additive color model.}$, the so called tristimulus values. \\

Pragmatically speaking, color spaces describes the range of colors a camera can see, a printer can print or a monitor can display. Thus, formally we can define it as a mapping a range of physically produced colors from mixed light to an objective description of color
sensations registered in the eye of an observer in terms of tristimulus values. 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{background/somatchingfunctions.png}
  \caption{Plots of our color matching functions we used for rendering}
  \label{fig:matchingfunction}
\end{figure}

Interpolating all measured tristimuli values gives us three basis functions, the CIE color matching functions $\overline{x}(\lambda)$, $\overline{y}(\lambda)$, $\overline{z}(\lambda)$. In figure $\ref{fig:matchingfunction}$ are the numerical description of the chromatic response of the observer. They can be thought of as the spectral sensitivity curves of three linear light detectors yielding the CIE Tristimulus values X, Y and Z. \\

The tristimulus values for a color with a spectral power distribution $I(\lambda)$, are given in terms of the standard observer by:

\begin{align}
    X= \int_{\Lambda} I(\lambda)\,\overline{x}(\lambda)\,d\lambda \nonumber \\
    Y= \int_{\Lambda} I(\lambda)\,\overline{y}(\lambda)\,d\lambda \nonumber \\
    Z= \int_{\Lambda} I(\lambda)\,\overline{z}(\lambda)\,d\lambda
\label{eq:tristimulusvalues}
\end{align}

Where $\lambda$, is the wavelength of the equivalent monochromatic light spectrum $\Lambda = [380nm, 780nm]$. Note taht it is not possible to build a display that corresponds to the CIE XYZ colorspace. For this reasons it is necessary to design other color spaces, which are physical realizable, offers efficient encoding, are perceptual uniform and have an intuitive color specification. There are simple conversions between XYZ color space, to other color space described as linear transformations.

\subsection{Spectral Rendering}
When rendering an image, most of the time we are using colors described in a certain RGB color space. However, a RGB colorspace results from a colorspace transformation of the tristimulus values, which themself are inherent to the human visual system. Therefore, many physically light phenomenon are poorly modeled when always relying on RGB colors for rendering. Using only RGB colors for rendering is alike we would assume that a given light source emits light of only one particular wavelength. But in reality this is barely the case. Spectral rendering is referring to use a certain wavelength spectrum, e.g. the human visible light spectrum, instead simply using the whole range of RGB values in order to render an illuminated scene. This captures the physical reality of specific light sources way more accurate. Keep in mind that, even when we make use of a spectral rendering approach, we have to convert the final spectra to RGB values, when we want to display an image on an actual display. 

\section{Wave Theory for Light and Diffraction}
\subsection{Basics in Wave Theory}
In order prepare the reader for physical relevant concepts used during later derivations and reasonings within this thesis, I am going to provide a quick introducation to the fundamental basics of wave theory and related concepts. In physics a wave describes a disturbance that travels from one location to another through a certain medium. The disturbance temporarly displaces the particles in the medium from their rest position which results in an energy transport along the medium during wave propagation. Usually, when talking about waves we are actually refering to a complex valued function which is a solution to the so called wave equation which is modeling how the wave disturbance proceeds in space during time. \\

There are two types of waves, mechanical waves which deform their mediums during propagation like sound waves and electromagnetic waves consiting of periodic oscilations of an electromagnetic field such as light for example. Like simplified illustrated in figure $\ref{fig:wavebasics}$, there are several properties someone can use and apply in order to compare and distinguish different waves:

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.65]{background/waveschematicimpr.png}
  \caption[sinewave]{Simplified, one dimensionaly real valued wave function$\footnotemark$, giving an idea about some important wave properties. We denote the crest of a wave as the hightest point relative to the equilibrium line (zero height along time axis) and similarly the trough as the lowest point.}
  \label{fig:wavebasics}
\end{figure}
\footnotetext{Image source: http://neutrino.ethz.ch/Vorlesung/FS2013/index.php/vorlesungsskript}

\begin{description}
  \item[Wavelength:] Is usually denoted by $\lambda$ and is a measure for the spatial distance from one point to another until the shape of a wave repeats
  \item[Amplitude:] Is denoted by $A$ and there are two possible interprations: First, it is a measure of the height from the equilibrium point to the heighest point of a crest on the wave or the lowest point of a trough. This mean the amplitude can be positive or negative. However, usually, someone is  just interested the absolute value of an amplitude, the magnitude of a wave. For light waves it is a relative measure of intensity or brightnes to other lught waves of the same wavelength. And secondly, it can be interpreted as a measure how much energy a wave carries wherate the greater the absoulte aplitute value, the bigger the amount of energy being carried.
  \item[Frequency:] Is a measure of the number of waves which are passing through a particular point in the propagation medium during a certain time and is denoted by $f$.
  \item[Phase:] Is denoted by $\phi$. Describes either the offset of initial position of a wave or the relative displacement between or among waves having the same frequency. Two waves two waves with same frequency are denoted by being in phase if they have the same phase. This means they line up everywhere. As a remark, we denote by $\omega$ the angular frequency which is equal $2\pi f$. 
\end{description}

A geometrical property of waves is their wavefront. This is either a surface or line along the path of wave propagation on which the disturbance at every point has the same phase. Three are basically three types of wavefronts: spherical-, cylindrical- and plane wavefront. If point in a isotropic medium is sending out waves in three dimensions, then the coresponding wavefronts are spheres, centered on the source point. Hence spherical wavefront is the result of a spherical wave, also denoted as a wavelet. Note that for electromagnetic waves, the phase is a poisition of a point in time on a wavefront cycle (motion of wave over a whole wavelength) whereat a complete cycle is defined as being equal 360 degrees.

\subsection{Wave Interference}
Next, after having seen that a wave is simply a traveling disturbance along a medium, having some special properties, someone could ask what happens when there are several waves travaling on the same medium. Especially, we are interested how these waves will interact with each other. In physics the term interference denotes the interaction of waves when they encounter each other at a point along their propagation medium. At each point where two waves superpose, their total displacement at these points is the sum of the displacements of each indiviudal wave at those points. Then, the resulting wave is having a greater or lower amplitude than each seperate wave and this we can interprete the interference as the addition operator for waves. Two extreme scenarios are illustrated in figure $\ref{fig:interferenceconcept}$. There are basically three variants of interferences which can occur, depending on how crest and troughs of the waves are matched up:

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.65]{background/interferenceconcept.png}
  \caption[interference]{Interference scenarios$\footnotemark$ when two waves waves meet: On the left handside, there is constructive interference and on the right handside there is destructive interference illustrated.}
  \label{fig:interferenceconcept}
\end{figure}
\footnotetext{Image source: \texttt{http://en.wikipedia.org/wiki/Interference\textunderscore(wave\textunderscore propagation)} } 

\begin{itemize}
  \item Either a crest of a wave meets a crest of another wave or similarly a trough meets a trough of another wave. This scenario is denoted as constructive interference and occurs at any location along the medium where the two interfering waves have a displacement in the same direction. This is equivalent like saying that the phase difference between the waves is a multiple of $2\pi$. Then the resulting amplitude at that point is being much larger than the amplitude of an individual wave. For two waves with an equal amplitude interfering constructively, the resulting amplitude is twice as large as the amplitude of an individual wave.
  \item Either a crest of a wave meets a trough of another wave or vice versa. This scenario is denoted as destructive interference and occurs at any location along the medium where the two interfering waves have a displacement in the opposite direction. This is like saying that the phase difference between the waves is an odd multiple of $\pi$. Then the waves completely cancel each other out at any point they superimpose.
  \item If the phase difference between two waves is intermediate between the first two scenarios, then the magnitude of the displacement lies between the minimal and maximal values which we could get from constructive interference.
\end{itemize}

Keep in mind that when two or more waves interfere which each other, the resulting wave will have a different frequency. For a wave, having a different frequency also means having a different wavelength. Therefore, this directly implies that a light of a different color, than its source waves have, is emitted. 

\subsection{Wave Coherence}
\label{sec:wavecoherence}
When considering waves which are traveling on a shared medium along the same direction, we could examine how their phase difference is changing over time. Formulating the changement of their relative phase as a function of time will provides us a quantitative measure of the synchronism of two waves, the so called wave coherence. In order to better understand this concept, let us consider a perfectly mathematical sine wave and second wave which is a phase-shifted replica of the first one. A property of mathematical waves is that they keep their shape over an infinity amount of moved wavelengths. In our scenario, both waves are traveling along the same direction on the same medium, like exemplarily illustrated in figure $\ref{fig:coherencesinsignal}$.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{background/coherencesinsignal.png}
  \caption{Two mathematical sine waves which are perfectly coherent which means that their phase difference is constant for every point in time.}
  \label{fig:coherencesinsignal}
\end{figure}

Taking the difference between the two sine waves from the previous figure yields always a constant number. Therefore, those two waves are said to be coherent and hence perfectly synchronous over time. Notice that this scenario is completly artificial since in nature there are no mathematical sine waves. Rather, the phase difference is then a fuction of time $p(t)$. The more coherent two waves are, the slower this function will change over time. 
In fact, two waves are said to be coherent if they are either of the same frequency, temporally in phase or have the same amplitude at every point in time.
Thus two waves are coherent if they are generated at the same time, having the same frequency, amplitude, and phase. Reversely, Waves are considered incoherent or also asynchronous if they have no stable phase difference. This means $p(t)$ is heavly varying over time. Coherence describes the effect of whether waves will tend to interfere with each other constructively or destructively at a certain point in time and space. Thus this is a property of waves that enables stationary interference. The more correlated two waves are, the higher their degree of coherence is. In physics coherence between waves is quantified by the corss-correlation function, which basically predicts the value of a second wave using the value of the first one. There are two basic coherence classifications:

\begin{itemize}
  \item Spatial coherence is dealing with the question of what is the range of distance between two points in space in the extend of a wave for which there is occuring a significant effect of interference when averaged over time. This is formally answered by considering the correlation between waves at different point in space. The range of distance is also denoted as the coherence area.
  \item Temporal coherence examines the ability of how well a wave will interfer with itself at different moments in time. Mathematically, this kind of coherence is computed by averaging the measured correlation between the value of the wave and the delayed version of itself at different pairs of time. The Coherence time denotes the time for which the propagating wave is coherent and we therefore can predict its phase using the correlation function. The distance a wave has traveled during the coherence time is denoted as the coherence length.
\end{itemize}


\subsection{Huygen's Principle}
Besides from a wave's phase and amplitude, also its propagation directly affects the interaction between different waves and how they could interfere with each other. This is why it makes sense to formulate a model which allow us to predict the position of a moving wavefront and how it moves in space. This is where Huygen's Principle comes into play. It states that any each point of a wavefront may be regarded as a point source that emits spherical wavelets in every direction. Within the same propagation medium, these wavelets traval at the same speed as their source wavefront. The position of the new wavefront results by superimposing all of these emitted wavelets. Geometrically, the surface that is tangential to the secondary waves can be used in order to determine the future position of the wavfront. Thesrefore, the new wavefront encloses all emitted wavelets. Figure $\ref{fig:huygensprinciple}$ visualizes Huygen's Prinicple for a wavfront reflected off from a plane surface.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{background/huygensprinciple.png}
  \caption[Huygen's Principle]{A moving wavefront (blue) encounters an obstacle (a surface in brown colors) and produces a new wavefront (green) as a result of superposition among all secondary wavelets.}
  \label{fig:huygensprinciple}
\end{figure}

\subsection{Waves Diffraction}
Revisting Hugen's Principle we know that each point on a wavefront can be considered as a source of a spherical wavelet which propagates in every direction. But what exactly happens when a wave's propagation direction is occluded by an object? What will be the outcome when applying Huygen's Principle for that case? An example scenario for this case is shown in figure $\ref{fig:wavediffraction}$. 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.65]{background/diffractiontransmissive.png}
  \caption[Diffracted Wave]{Illustation$\footnotemark$ of a diffraction scenario in which a plane wavefront passes through a surface with a certain width and how the wave will be bent, also showing the intensity of the resulting wave.}
  \label{fig:wavediffraction}
\end{figure}
\footnotetext{Image source:\texttt{http://cronodon.com/images/Single\textunderscore slit\textunderscore diffraction\textunderscore 2b.jpg} } 

Whenever a propagating wavefront is partially occluded by an obstacle, the wave is not only moving in the direction along its propagation, but is also bent around the edges of the obstacle. In physics, this phenomenon is called diffraction. Waves are diffracted due to interference which occures among all wavelets when applying Huygen's Principle for the case when a wavfront hits an obstacle. Generally, the effect of diffraction is most expressed for waves whose wavelength is roughly similar in size to the dimension of the occluding object. Conversely, if the wavelength is hardly similar in size, then there is almost no wave diffraction perceivable at all. This relationship between the strength of wave diffraction and wavelength-obstacle-dimensions is conceptually illustrated in figure $\ref{fig:diffractionrelationshipdimension}$ when a wave is transmitted through a surface. A reflective example is provided in figure $\ref{fig:huygensprinciple}$.

\begin{figure}[H]
  \centering
  \subfigure[W $\ll$ $\lambda$]{
  \includegraphics[scale=0.7]{background/Aa2l.png}
    \label{fig:a1}
  }
~
  \subfigure[W $\approx$ $2 \lambda$]{
    \includegraphics[scale=0.7]{background/Aastl.png}
    \label{fig:a2}
  }
~
  \subfigure[W $\approx$ $6 \lambda$]{
    \includegraphics[scale=0.7]{background/Aa6l.png}
    \label{fig:a3}
  }
  \caption[diffractiondiemsnion]{Illustration$\footnotemark$ of how the effect of diffraction changes when a wave with wavelength $\lambda$ propagates through a slit of width equal $W$.}
  \label{fig:diffractionrelationshipdimension}
\end{figure}
\footnotetext{Image taken from:\texttt{http://neutrino.ethz.ch/Vorlesung/FS2013/index.php/vorlesungsskript}, chapter 9, figure 9.14 } 

In everday's life, we can see the direct outcome of the effect of wave diffraction in form of structural colors. There are examples from nature such as the irridecent colors on various snake skins as well as artificial examples such as the colorful patterns notable when having a close look at an illuminated compact disc. All in common having having a surface made of very regular nanostructure which is diffracting an incident light. Such a nanostructure which exhibits a certain degree of regularity is also denoted as diffraction grating. More about this in section $\ref{sec:diffractiongrating}$.

\section{Stam's BRDF formulation}
\label{sec:sumstam}
The theoretical foundation of this thesis is based on the pioneering work of J.Stam who derived in his paper about Diffraction Shader$\cite{diffstam}$ a BRDF which is modeling the effect of far field diffraction for various analytical anisotropic reflexion models, relying on the so called scalar wave theory of diffraction for which a wave is assumed to be a complex valued scalar. \\

It's noteworthy, that Stam's BRDF formulation does not take into account the polarization of the light. Fortunately, light sources like sunlight and light bulbs are unpolarized. The principal idea behind J. Stam's approach is illustrated in figure $\ref{fig:meaningofstamsapproach}$. 
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{background/stamsapproachsmeaning2.png}
  \caption[Idea behind Stam's approach]{Illustration of idea behind Stam's approach: Integartion over all secondary sources according to Huygen's principle resulting from an incident wave will give us an identity for the total contribution at a certain point in space.}
  \label{fig:meaningofstamsapproach}  
\end{figure}

An incident wave $p_i$ from a a light source encounters a surface, representing a diffraction grating. According to Huygen's Principle, at any point $i$ on the grating, at which the incident wave meets the grating, a secondary, spherical wavelet $p_{r,i}$ will be emitted. A viewer, in the figure indicated by a gray circle, will perceive the superimposed contributation of all wavelets along the surface $S$ (in the figure indicated by a integration symbol), which wil directly follow the laws of wave interference. Therefore the resulting color which an observer sees is the final radiance at that point which itself is affected by stationary interference of all emitting secondary sources due to Huygen's principle. \\

A further assumption in Stam's Paper is, that the emanated waves from the source are stationary, which implies the wave is a superposition of independent monochromatic waves. This further implies that each wave is associated to a definite wavelength $\lambda$. However, directional light sources, such as sunlight fulfills this fact and since we are using these kinds of light sources for our simulations, Stam's model can be used for our modeling purposes. \\

The main idea of his model is the formulate a BRDF as the Fourier Transform applied on the given height field, representing a surface like shown in figure $fig:geometricsetup$. The classes of surfaces his model is able to support either exhibit a very regular structure or may be considered as a superposition of bumps forming a periodic like structure. Therefore, the surfaces he is dealing with can either be modeled by probabilistic distributions or have a direct analytical representation. Both cases allow him to derive an analytical solution for his BRDF model.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{background/stamsinputp.png}
  \caption[Stam's geometrical setup]{Illustration$\footnotemark$ of geometrical setup of Stam's approach where $\omega_i$ is a direction, pointing towards the light source, $\omega_r$ points towards the camera, $n$ is the surface normal, $(u,v,w)$ are the components of the vector $-\omega_i - \omega_r$.}
  \label{fig:geometricsetup}  
\end{figure}
\footnotetext{Modified image which originally has been taken from D.H. Dhillon's poster$\cite{diffourp}$.} 

The direction vector of the secondary wavelet can be computet by taking the difference between the incident and viewing direction like shown in equation $\ref{eq:uvw}$:
\begin{equation}
  (u,v,w) = -\omega_i - \omega_r 
\label{eq:uvw}
\end{equation}
These coordinates will later be used in order to compute the total contributation of all secondary sources used in Stam's BRDF in equation $\ref{eq:mainstam}$. For simplification, let us introduce an auxillary function $\Phi$ defined in equation $\ref{eq:heightfieldphase}$, which models the phase of a wave from the provided height field.

\begin{equation}
  \Phi(x,y) = \frac{2 \pi}{\lambda} w h(x,y) 
\label{eq:heightfieldphase}
\end{equation}

Then, any secondary wavelet $p$ which is emitted off from the given surface will be equal:
\begin{equation}
  p(x,y) = e^{i\Phi(x,y)} 
\label{eq:px}
\end{equation}

using the idea presented for figure $\ref{fig:meaningofstamsapproach}$ and performing all mathematical steps shown in the appendix $\ref{chap:stamsderivations}$, will lead us to the final BRDF representation, modeling the total contribution of all secondary sources reflected off the the provided surface $h$:
\begin{equation} 
  BRDF_{\lambda}(\omega_i, \omega_r) = \frac{k^2 F^2 G}{4\pi^2 A w^2} \langle \left|P(ku, kv)\right|^2\rangle
\label{eq:mainstam}
\end{equation}

where $F$ denotes the Fresnel coefficient and $G$ is the so called geometry term$\footnote{The geometric Terms expresses the correction factor to perform an integration over an area instead over a surface. For further information, please have a look at \texttt{http://en.wikipedia.org/wiki/Surface\textunderscore integral}, and read the definition about \emph{surface element}}$ which is equal to: 
\begin{equation}
  G =\frac{(1 + \omega_i \cdot \omega_r)^2}{cos(\theta_i)cos(\theta_r)}
\label{eq:geometricterm}
\end{equation}

\label{sec:electricalengeneeringftconvention}
One last word about the term Fourier Transform Stam uses in his derivation: Conventionally, following the definitions in Mathematics of the Fourier Tranformation, we are dealing with the inverse Fourier Transformation. However, especially in electrical engeneering, it is quite common to define this inverse Fourier Transformation by the Fourier Transformation. The reason behind this lies in the fact that we simply could substitude the minus sign like the following equation $\ref{eq:signchangementconvention}$:

\begin{align}
\mathcal{F}_{FT}\{f\}(w) 
& = \int_{\mathds{R}^n} f(x)e^{-iwt} dt \nonumber\\
& = \int_{\mathds{R}^n} f(x)e^{i\hat{w}t} dt \nonumber\\
& = \mathcal{F}^{-1}_{FT}\{f\}(w)
\label{eq:signchangementconvention}
\end{align} 
where $\hat{w}$ is equal $-w$. \\

The height fields we are dealing with in this work are, however, natural gratings, containing a complex shaped nano-structure and hence far apart from being very regularly aligned. The reason why Stam's approach in its current form is not suitable for our purpose is twofold: First his approach does not capture the complexity of natural gratings accurately well enoug when relying on his his statistical approaches and secondly it is way too slow in order to be usable for interactive rendering since his BRDF needs an evaluation of a Fourier Transform for every directional changement. \\

In the follwing a brief comparission between Stam's$\footnote{A reference implementation of Stam's Diffraction Shader\cite{diffstam} is provided by Nvidia's GPU Gems at \texttt{http://http.developer.nvidia.com/GPUGems/gpugems\textunderscore 08.html}}$ and our final approach using two different kinds of gratings, a synthetic, regularly aligned grating and a natural, complex structured grating. These gratings are shown in figure $fig:stameggratings$.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{background/gratingsstamreworked.png}
  \caption[Comparing Stam's apporach: Gratings]{Alignment of nanostructures in diffraction gratings. On the left a complex, natural grating of the Elaphe snake species and on the right a synthetic, very regularly aligned grating of a CD.}
  \label{fig:stameggratings}  
\end{figure}

Figure $\ref{fig:stameggratingsgoodeg}$ shows an example of a case where Stam's approach performs well. Considering the red-line in the figure we notice that the nano-scaled structures of a compact disc are very regularly aligned along the surface. Tracks of a CD are uniformally spaced and bumps along a track are distributed according to the poisson distribution$\footnote{See \texttt{http://en.wikipedia.org/wiki/Poisson\textunderscore distribution}}$. All angles of the diffraction pattern look the same as in the images produced by our apporach.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{background/stamgoodeg.png}
  \caption[Comparing Stam's apporach: Good Example]{Comparission of our approach against a reference implementation of Stam's provided by Nvida Gem. For synthetic diffraction gratings, which have a very regular structure, Stam's apporach is doing well. All angles of the diffraction pattern look the same as in the images produced by our apporach.}
  \label{fig:stameggratingsgoodeg}  
\end{figure}

Figure $\ref{fig:stameggratingsbadeg}$ shows an attempt to reproduce real structural colors on the skin of the Xenopeltis snake using our method and comparing it to Stam's approach. Even the results of Stam might look appropriate, there are some differences notable such missing colors close to speclar regions, or such as the colordistribution which is rather discrete in Stam's approach. Furthermore, Stam's approach has at some places color contribution, where it should not have.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{background/stambadeg.png}
  \caption[Comparing Stam's apporach: Bad Example]{Comparission of our approach(on the right) against a reference implementation of Stam's(on the left) provided by Nvida Gem by trying to render a reproduction of a real Elaphe skin (center) under similar lightning and viewing conditions. For natural diffraction gratings, which have a rather complex structure, Stam's apporach is doing rather bad.}
  \label{fig:stameggratingsbadeg}  
\end{figure}

In the next chapter we are going to adapt Stam's BRDF model such that it will be able to handle the kind of surfaces we are deling with and even will have a runtime complexity which allows to perform interactive rendering.

